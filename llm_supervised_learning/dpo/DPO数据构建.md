## DPO数据结构

DPO使用偏好数据对优化策略模型，每个数据包括以下部分：

- **Prompt（输入）**：用户提出的指令或问题，例如 “如何评估个人信用？”

- **Chosen（更优输出）**：人类标注者认为更符合偏好的回答，例如：“信用评估通常基于信用评分，如征信，同时会考虑还款记录、负债比例等因素。”

- **Rejected（较差输出）**：人类标注者认为不符合偏好的回答，例如：“信用评估就是看看你有没有钱，钱多就信用好。”

## 数据集构建流程

DPO数据集的构建是一个系统化工程，通常经过**录制（采集）、清洗（预处理）、标注（偏好判定）、配对**这几个生成数据集。

### 1. 录制（数据采集）

#### 1.1 一般方法

目标是获取原始数据样本，覆盖多领域、多任务类型，确保数据多样性。  可以通过以下方法获取：

- 真实用户数据：
  
  - 从对话系统、代码编辑器等场景中采集用户与模型的交互日志。

- 人工生成：  
  
  - 人工编写优质数据

- 自动生成：  
  
  - 模型生成：用基础模型（如GPT）生成多个候选响应，再筛选优质样本。  
  - 规则生成：针对特定任务（如数学题）编写规则生成正确/错误答案。例如，生成10个快速排序的实现，通过单元测试筛选正确/错误代码。  

- 开源数据

#### 1.2 特殊情况

人工编写偏好数据集的成本较高。如果已经有一批数据，但是数据不够完整，可以分情况合成。

- 有prompt，但是没有chosen和rejected
  
  可以调用其他性能更好的模型生成多条数据，也可以用规则的方式生成，参考前文描述。最后，从中选择chosen和rejected。

- 只有chosen或只有rejected
  
  以现有结果为基准，让大模型生成可能的prompt，然后组合现有结果和新prompt，让模型生成新的结果。

### 2. 清洗（预处理）

- 质量过滤
  
  - 规则过滤：通过正则或其他方式，删除具有不良模式的数据，例如连续重复出现的表情、标点符合、短语。
  
  - 语言质量：过滤语法错误、乱码、非完整句子（如用NLP工具检测语言流畅度）。 
  
  - 内容质量：过滤有害内容（如暴力、歧视）、事实错误（如用搜索引擎验证科学知识）。

- 重复去除
  
  - 精确去重：计算响应的哈希值，移除完全相同的样本。  
  
  - 语义去重：用Sentence-BERT等模型计算响应嵌入向量，按内容相似度移除重复样本。  
  
  - 主题分类：对数据进行分类，保留和目标任务关联度高的数据

- 格式统一
  
  - 标准化字段名（如`prompt`统一为小写）。  
  
  - 统一编码（如UTF-8）、换行符（如`\n`）。  

- 长度控制
  
  - 过滤过长/过短文本 

### 3. 标注（偏好判断）

**目标**：从候选响应中选出最优（Chosen）和最差（Rejected）响应，形成偏好对。  

- 标注方法
  
  - 人工标注：  标注员根据任务要求（如正确性、安全性）选择最优/最差响应。
  
  - AI标注：用性能更强的模型（如GPT-4、Claude）对候选响应评分，选择评分最高/最低的响应。
  
  - 混合标注：先用AI Judge预筛选，再由人工复核关键样本（如安全对齐场景）

- 标注规范
  
  - 根据不同任务制定不同的规范，例如对话类任务可以关注相关性、无害性，代码生成任务可以关注正确性、时间复杂度、代码风格
  
  - 标注一致性：关注数据是否按统一的要求进行标注

### 4. 配对（生成偏好对）

**目标**：将标注后的`chosen`和`rejected`响应与原始`prompt`组合，形成DPO训练所需的偏好对。  

- 配对方法
  
  - 一对一：选择最好的结果和最差的结果组成偏好对
  
  - 多对多：从结果中按质随机抽取两个结果，并按质量确认偏好关系

- 相似度过滤
  
  - 避免`chosen`和`rejected`过于相似（如仅修改一个标点符号），否则模型难以学习有效偏好。

- 领域平衡
  
  - 确保不同领域（如医学、法律）的样本比例合理，避免模型偏向某一领域。  
