**DPO**（Direct Preference Optimization）是一种针对大型语言模型（LLMs）的优化方法，旨在通过人类偏好数据直接调整模型参数，无需显式构建奖励模型或复杂强化学习过程。 ‌DPO最早源自2023年Rafael Rafailov发表的《Direct Preference Optimization: Your Language Model is Secretly a Reward Model》，更多请细节参考原论文，访问地址 ﻿https://arxiv.org/pdf/2305.18290 。

## 核心特点

在DPO之前，基于PPO（Proximal Policy Optimization）的RLHF一直是主流的对齐算法。但是PPO存在许多问题：

- **稳定性**：PPO有非常多影响效果的超参，并且训练过程耦合4个模型，训练的不稳定相对较差

- **效率与资源**：RLHF的训练分两个阶段，需要先训练奖励模型，然后同时加载4个模型进行训练，对算力和显存的占用较高

DPO通过收集用户对模型输出的偏好数据（如接受/拒绝的答案对），直接优化模型参数以对齐人类偏好，避免了传统 RLHF （ 强化学习从人类反馈 ）方法中复杂的奖励模型训练和强化学习优化过程。

其核心优势包括：

- ‌**简化训练流程**：无需显式拟合奖励模型，直接通过偏好数据优化策略；

- ‌**提高稳定性‌**：绕过强化学习的不稳定性，降低训练风险；

- **效率提升**：通过最大似然估计优化策略，减少计算成本。 ‌训练过程只需要加载一个模型（参考模型的结果可以提前录制）。

## 原理简述

DPO基于Bradley-Terry模型分析偏好数据，利用KL散度衡量模型输出与人类偏好的差异，并通过梯度更新调整参数。具体流程包括：

1. ‌**偏好采样‌**：收集用户对模型输出的接受/拒绝反馈；

2. ‌**奖励函数推导‌**：通过偏好数据推导奖励函数；

3. ‌**策略优化‌**：直接优化策略模型参数以最大化奖励函数。 ‌

该方法已被应用于语言模型优化，并展现出与RLHF相当的性能，但训练过程更高效且稳定。

![](./image/2025-07-22-20-23-21.png)

## Bradley-Terry模型

DPO的奖励函数设计继承了Bradley-Terry模型，后者是一种用于分析成对比较（pairwise comparison）数据的统计模型，由Ralph A. Bradley和Milton E. Terry于1952年提出。其核心目标是通过量化不同个体或对象的相对强度，预测在直接对抗中某一方战胜另一方的概率。该模型广泛应用于体育排名、消费者偏好研究、搜索引擎排序、电竞匹配系统等领域。

### 1. 基本假设

- **隐式强度参数**：每个对象（如球队、产品）被赋予一个正实数参数 λ<sub>i</sub> 或者 θ<sub>i</sub>，表示其相对实力或偏好强度。

- **胜负概率公式**：  
  对于两个对象  i  和  j ，模型假设 i 战胜 j 的概率为：  
  ![](./image/2025-07-22-20-28-16.png)
  也可以通过重参数化（引入对数线性模型）：  
  
  P(i > j) = σ( λ<sub>i</sub> - λ<sub>j</sub> )
  
  这与逻辑回归的Sigmoid函数形式一致。

### 2. 参数估计

通过最大化观测数据的似然函数，就可以估计参数。对于胜负记录矩阵 W ，其中 W<sub>ij</sub> 表示i战胜j的次数。似然函数为：  
![](./image/2025-07-22-20-36-26.png)

## DPO的奖励函数

### 1. 奖励函数定义

结合BT模型可以将人工偏好的分布定义为

![](./image/2025-07-23-11-01-07.png)

r(x, y) 奖励函数是奖励函数，可表示为策略模型和参考模型的对数概率比，公式如下：  

![](./image/2025-07-22-20-54-08.png)

其中，π<sub>θ</sub>​为当前策略模型，π<sub>ref</sub>​为参考模型（例如预训练模型）。π<sub>θ</sub>(y|x)表述输入是x时，模型输出y的概率​，也就是条件概率。β为控制奖励强度的超参数。实际上，DPO的奖励函数中还存在一个难以计算的未知项，但在DPO的后续计算过程中，该项可以被抵消，可以参考后文。

### 2. 与显式奖励模型的对比

- **显式奖励模型（EX-RM）**：  
  通过独立训练的线性头预测奖励分数，作为“裁判”模型，直接对输出进行评分。
- **隐式奖励模型（IM-RM）**：  
  奖励信号隐含于策略模型与参考模型的概率差异中，模型本身既是“选手”又是“裁判”，避免了显式奖励模型的复杂性。

## 强化学习的目标

DPO是对传统强化学习范式改进后的算法。在强化学习中，学习的目标是找到最优的策略模型π，使目标函数最大，如下

![](./image/2025-07-23-12-44-40.png)

其中，π<sub>ref</sub>表示参考模型，参考模型是固定的，和训练参数无关。D<sub>KL</sub>表示两个模型间的KL散度，也就是两个模型间的概率分布差异。换句话说，上述目标是在控制策略模型、参考模型之间差异的前提下，实现奖励期望的最大化。

对目标函数做变换，得到：

![](./image/2025-07-23-12-52-04.png)

其中

![](./image/2025-07-23-12-52-51.png)

上面的函数只和 x 以及参考策略 π<sub>ref</sub> 有关，不依赖策略 π 。因此，可以定义最优策略的形式为

![](./image/2025-07-23-12-58-58.png)

上式是一个有效的概率分布，因为对所有的 y 有 π<sup>*</sup>(y|x) ≥ 0，并且 Σ<sub>y</sub> π<sup>*</sup>(y|x) = 1。因为Z(x)和y无关，可以将目标改写为：

![](./image/2025-07-23-13-04-50.png)

又因为Z(x)和策略模型π无关，所以最小化上式相当于最小化第一项的KL散度。当两个分布相等时，KL散度取得最小值，最小值为0，此时得到的解就是最优解，即：

![](./image/2025-07-23-13-06-19.png)

对上式做变换得到最优奖励的公式：

![](./image/2025-07-23-13-12-07.png)

我们知道，DPO通过BT模型定义了偏好样本对之间的概率

![](./image/2025-07-23-11-01-07.png)

将最优奖励代入偏好概率的公式中，得到

![](./image/2025-07-23-13-13-38.png)

至此，我们发现难以计算的Z(x)被抵消，这就是为什么DPO将奖励函数定义为

![](./image/2025-07-22-20-54-08.png)

## DPO的损失函数

对于满足 p<sup>*</sup> 的偏好数据集 D = {x<sup>(i)</sup>, y<sub>w</sub><sup>(i)</sup>, y<sub>l</sub><sup>(i)</sup>}<sup>N</sup><sub>i=1</sub>，可以用奖励模型拟合这个数据集的分布，将问题建模为二分类问题。因此，对于数据集D中的数据，可以认为这是一系列随机事件，将似然函数定义为：

![](./image/2025-07-23-11-33-56.png)

即

![](./image/2025-07-23-11-35-24.png)

那么，目标就是使这些随机事件的联合概率最大，也就是让似然函数的值最大。为了便于计算，直接取负对数将上述公式展开：

![](/Users/wenyuexin/Library/Application%20Support/marktext/images/2025-07-23-11-37-37-image.png)

然后，将偏好概率p代入二分类的似然函数，就可以得到DPO的负对数似然损失，如下

![](./image/2025-07-22-21-02-58.png)

其中

- σ为sigmoid函数，值域是(0, 1)

- β为超参数，用于控制奖励强度

- 偏好数据集D中，x是输入，y<sub>w</sub>即y<sub>win</sub> 是模型的优质回答，y<sub>l</sub>即y<sub>lose</sub> 是模型的劣质回答

- DPO的目标函数是 J(θ) = -L<sub>DPO</sub>(θ)

可以看出，上述Loss中包含了 r(x, y<sub>w</sub>) 和 r(x, y<sub>l</sub>)，通过两者的差值计算二分类的损失。DPO的奖励来自于策略模型和参考模型，而不是独立的奖励模型，由于跳过了奖励模型的训练，直接优化策略模型，这就是DPO中Direct的含义。

## DPO的梯度

得到损失函数后，需要求解梯度。标量场中某点的梯度是一个向量，方向指向该点方向导数最大的方向，大小是最大方向导数的值。借助梯度可以按预期的更新模型参数。DPO损失函数求梯度如下

![](./image/2025-07-23-20-29-53.png)

这里引入一个辅助变量u

![](./image/2025-07-23-20-32-36.png)

将u代入公式

![](./image/2025-07-23-20-34-42.png)

我们知道，sigmoid函数的满足 

```
σ‘(x) = σ(x)[1 - σ(x)]
σ(-x) = 1 - σ(x)
```

因此，得到最终的梯度

![](./image/2025-07-23-20-48-35.png)

使用奖励函数r(x, y)对上式进行替换，可以得到以下形式

![](./image/2025-07-23-20-50-23.png)

可以看出，梯度的后半部分可以提升优质回答的概率，并降低劣质回答的概率。公式的前半部分存在动态系数σ[r<sub>θ</sub>(x,y<sub>l</sub>) - r<sub>θ</sub>(x,y<sub>w</sub>)]，这个系数通过劣质回答和优质回答的奖励差异，调整梯度的更新幅度。

![](./image/2025-07-23-20-58-34.png)

有了梯度之后，就可以使用梯度下降法更新模型参数，其中η是学习率：

θ ← θ - η · ∇<sub>θ</sub>L<sub>DPO</sub>(π<sub>θ</sub>; π<sub>ref</sub>)

## DPO的训练流程

模型的整体流程，通常先经过预训练、SFT，然后采用DPO。策略模型和参考模型一般直接复制SFT模型，但也可以选择比当前SFT模型更强的模型，此时需要关注策略模型和训练模型之间的匹配程度，主要考虑KL散度和数据分布。如图：

![](./image/2025-07-22-21-35-15.png)

其中，DPO的训练流程包含数据收集、模型推理、计算动作概率和损、反向传播、更新参数等步骤，如下图：

![](./image/2025-07-22-21-45-55.png)

## 超参对效果的影响

参数β是DPO（直接偏好优化）目标函数中的关键超参数，直接控制隐式奖励信号的强度，进而影响模型优化的方向、速度和稳定性。该参数在DPO中的作用类似于其在RLHF中的作用。体现在以下维度：

### 1. 奖励信号的缩放效应

DPO的隐式奖励函数为：  

![](./image/2025-07-22-20-54-08.png)
β通过缩放对数概率比，直接决定奖励值的幅度。  

- **大β值**：放大奖励差异，使模型更关注高奖励输出（如用户明确偏好的回复）。  
- **小β值**：压缩奖励差异，使模型对偏好对的区分度降低，可能忽略细微偏好信号。

### 2. 训练动态的调控

- **梯度更新强度**：  
  β影响损失函数对模型参数的梯度大小。  
  
  - **大β值**：梯度更新更剧烈，加速模型向高奖励输出收敛，但可能引发训练不稳定（如损失震荡）。  
  - **小β值**：梯度更新更平缓，训练过程更稳定，但收敛速度可能变慢。

- **KL散度约束的平衡**：  
  如果将KL散度引入训练，总目标函数为：  
  ![](./image/2025-07-22-22-06-10.png)
  β与λ共同作用，控制模型偏离参考模型的程度。  
  
  - **大β值**：需配合较小的λ，避免模型过度偏离参考模型（否则可能因奖励信号过强导致过拟合）。  
  - **小β值**：可适当增大λ，利用KL散度约束保持模型稳定性。

### 3. 模型性能的直接影响

简单概括

- **偏好对齐能力**：  
  
  - **大β值**：模型更积极学习偏好数据，可能提升在偏好对上的准确率（如对话系统中的回复相关性）。  
  - **小β值**：模型可能保留更多参考模型的语言能力，但偏好对齐效果减弱（如生成内容更流畅但偏离用户意图）。

- **泛化能力**：  
  
  - **大β值**：可能过拟合训练数据中的偏好对，导致在未见数据上表现下降（如新用户群体的偏好未被充分覆盖）。  
  - **小β值**：模型更保守，泛化能力更强，但可能牺牲部分偏好对齐精度。

### 4. 调优策略与最佳实践

- 渐进式调优法
  
  1. **初始设置**：从β=0.1开始，确保模型基本语言能力不受损。  
  2. **逐步增大β**：每次增加0.1~0.2，观察偏好对齐指标（如准确率、胜率）的变化。  
  3. **监控稳定性**：若损失函数出现震荡或模型性能下降，适当减小β或增大λ。  

- 基于任务复杂度的启发式调优
  
  1. **简单任务**（如二分类偏好）：β=0.5~1.0，奖励信号明确，需快速收敛。  
  2. **复杂任务**（如长文本生成）：β=0.1~0.3，保持模型探索能力，避免过拟合。  

### 5. 超参影响的总结

参数β是DPO中调控奖励信号强度、训练动态和模型性能的核心超参数。其取值需根据任务类型、数据规模和计算资源，通过实验逐步调整，以平衡偏好对齐能力与模型稳定性。

## DPO对模型能力的影响

SFT可以提供模型对指令的跟随能力、对话能力。DPO可以在模型具备一定对话能力后，让模型的输出结果更加符合人类偏好。

下图来自论文 https://arxiv.org/pdf/2404.14723 ，Mistral是法国一家公司发布的大模型。可以看出，与直接使用DPO、KTO对齐模型偏好相比，先做SFT在做偏好对齐有更好的效果。

![](./image/2025-07-22-22-31-04.png)

deepseek团队曾经在deepseek 67b chat上做对比实验，评估DPO前后模型基础能力的变化，论文地址 https://arxiv.org/pdf/2401.02954 。结果如下（参考最后两行的数据）：

![](./image/2025-07-22-23-04-48.png)
