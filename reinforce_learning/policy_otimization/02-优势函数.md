## 一、优势函数的定义与作用

### **1. 定义**

优势函数（Advantage Function）衡量在给定状态下执行某个动作相对于当前策略平均收益的优势，数学表达式为：  

$$
A(s,a) = Q(s,a) - V(s) 
$$

其中：

- Q(s,a) ：动作价值函数，表示在状态 s  下执行动作 a 的期望累积奖励。
- V(s) ：状态价值函数，表示在状态 s 下按当前策略行动的平均期望奖励。

### **2. 物理意义**

- **正值**：动作 a 优于平均水平，应被鼓励。
- **负值**：动作 a 劣于平均水平，应被抑制。

### **3. 作用**

- **减少方差**：通过对比动作与平均水平的差异，降低策略梯度估计的方差。
- **指导策略更新**：优化算法（如PPO、A2C）利用优势函数调整策略，使其更倾向于选择高优势动作。

## 二、AC算法及其变种

### 1. 基础AC算法（Actor-Critic）

#### **核心机制**

- **Actor（策略网络）**：输出动作概率分布，通过策略梯度更新参数。
- **Critic（价值网络）**：评估状态价值 $ V(s) $，结合优势函数指导Actor更新。

#### 优势函数

通过 $ A(s,a) = Q(s,a) - V(s) $ 计算动作优势，优化策略梯度方向。

#### **特点**

- 结合策略梯度与价值函数，平衡探索与利用。
- 适用于离散和连续动作空间，但传统AC方差较高。

### 2. A2C（Advantage Actor-Critic）

#### **改进点**

- **同步并行**：多个Worker同步更新全局网络，提高数据利用率。
- **优势函数**：通过 $ A(s,a) = Q(s,a) - V(s) $ 减少方差，提升稳定性。

#### **特点**

- 适用于离散动作空间（如Atari游戏）。
- 代码实现中，通过多线程并行采样，同步更新参数。

### 3. A3C（Asynchronous Advantage Actor-Critic）

#### **改进点**

- **异步并行**：多个Worker异步更新全局网络，加速训练。
- **探索效率**：不同Worker探索不同路径，避免局部最优。

#### **特点**

- 硬件友好，充分利用多核CPU资源。
- 适用于高维度环境（如机器人控制）。

### 4. DDPG（Deep Deterministic Policy Gradient）

#### **改进点**

- **确定性策略**：输出连续动作，结合Q函数估计优势。
- **经验回放与目标网络**：稳定训练过程，处理连续动作空间。

#### **优势函数应用**

- Critic通过TD误差更新Q值，间接计算优势函数。

#### **特点**

- 专为连续动作空间设计（如机械臂控制）。
- 稳定训练，但样本效率较低。

### 5. PPO（Proximal Policy Optimization）

#### **改进点**

- **策略剪辑**：通过限制策略更新步长（如 $ \text{clip}(\pi_{\text{new}}/\pi_{\text{old}}, 1-\epsilon, 1+\epsilon) $），避免过大更新。
- **优势函数**：结合GAE估计优势，平衡偏差与方差。

#### **特点**

- 稳定性高，适用于复杂任务（如自动驾驶）。
- 超参数敏感（如 $ \epsilon $ 值需精细调整）。

### 6. SAC（Soft Actor-Critic）

#### **改进点**

- **熵正则化**：在奖励函数中加入熵项，鼓励探索。
- **双Q网络**：减少Q值高估问题。

#### **优势函数应用**

- 优势函数中包含熵项，平衡探索与利用。

#### **特点**

- 适用于连续动作空间，探索效率高。
- 调参复杂，但开源实现成熟（如OpenAI Baselines）。

## **三、算法对比与适用场景**

| **算法**   | **优势函数核心作用**      | **特点**               | **适用场景**         |
| -------- | ----------------- | -------------------- | ---------------- |
| **AC**   | 衡量动作相对优势，指导策略更新   | 基础框架，方差较高，适用性广       | 简单离散/连续任务        |
| **A2C**  | 同步并行+优势函数，减少方差    | 数据利用率高，稳定性提升，支持复杂环境  | Atari游戏、多线程任务    |
| **A3C**  | 异步并行+优势函数，提升探索效率  | 硬件友好，探索效率高，避免局部最优    | 机器人控制、高维度环境      |
| **DDPG** | 确定性策略+优势函数，处理连续动作 | 专为连续空间设计，稳定训练，输出具体动作 | 机械臂控制、连续动作游戏     |
| **PPO**  | 策略剪辑+优势函数，平衡稳定与效率 | 稳定性最优，超参数敏感，适用于高维度决策 | 自动驾驶、资源管理、复杂策略游戏 |
| **SAC**  | 熵正则化+优势函数，鼓励探索    | 探索效率高，调参复杂，开源实现成熟    | 连续动作空间、需要平衡探索的任务 |

## **四、广义优势估计**

广义优势估计，Generalized Advantage Estimation，简称GAE。

### **1. 核心原理**

#### 1.1 定义

GAE通过多步时间差分（TD）误差的加权平均，平衡优势函数估计的偏差与方差。数学表达式为：  

$$
A_t^{\text{GAE}(\gamma, \lambda)} = \sum_{k=0}^{\infty} (\gamma \lambda)^k \delta_{t+k} 
$$

其中，$ \delta_t = r_t + \gamma V(s_{t+1}) - V(s_t) $ 是单步TD误差，$\gamma $控制未来奖励权重，$\lambda$ 制偏差与方差的平衡。虽然公式中求和累计到了正无穷，但是迭代过程中如果遇到了终止步，可以提前结束，此时终止状态的价值为0，然后可以从末端往前递推出TD误差，然后又TD误差递推出GAE。

#### 1.2 关键推导

- **多步优势估计**：n步优势估计为 
  
  $$
  A_t^{(n)} = \sum_{k=0}^{n-1} \gamma^k \delta_{t+k} 
  $$

- **广义加权平均**：GAE将不同步数的优势估计通过 $ \lambda $ 加权组合，递归形式为： 
  
  $$
  A_t^{\text{GAE}} = \delta_t + \gamma \lambda A_{t+1}^{\text{GAE}} 
  $$

### **2. 参数作用**

- **$ \lambda $**：  
  
  - $ \lambda \to 1 $：方差大，偏差小（接近蒙特卡洛估计）。  
  - $ \lambda \to 0 $：方差小，偏差大（接近单步TD估计）。  
  - 实际中，$ \lambda $ 常取 $ 0.9 \sim 0.99 $

- **$ \gamma $**：  
  
  - 控制未来奖励的折扣因子，直接影响价值函数的估计。

### **3. 实际应用**

#### 3.1 策略梯度更新

$$
\nabla J(\theta) = \mathbb{E} \left[ \sum_{t=0}^T A_t^{\text{GAE}} \nabla_\theta \log \pi_\theta(a_t | s_t) \right] 
$$

GAE替代原始优势函数，提供更稳定的梯度估计。

#### 3.2 优势归一化

为减少方差，常对GAE估计进行标准化：  

$$
\hat{A}_t = \frac{A_t^{\text{GAE}} - \mu_A}{\sigma_A} 
$$

其中 $ \mu_A $ 和 $ \sigma_A $为批量数据的均值和标准差。

### **4. 代码示例（伪代码）**

```python
def compute_gae(rewards, values, gamma=0.99, lambda_=0.95):
    batch_size = len(rewards)
    advantages = np.zeros(batch_size)
    last_advantage = 0
    for t in reversed(range(batch_size)):
        delta = rewards[t] + gamma * values[t+1] - values[t]
        advantages[t] = delta + gamma * lambda_ * last_advantage
        last_advantage = advantages[t]
    return advantages
```

## **五、总结**

- **优势函数共性**：所有变种均通过优势函数评估动作质量，核心差异在于并行机制、策略类型（确定性/随机性）及更新约束。  
- **选择建议**：  
  - **离散动作**：优先A2C/A3C。  
  - **连续动作**：DDPG或SAC。  
  - **高稳定性需求**：PPO。  
  - **资源受限环境**：A3C（多核CPU）。  
- **GAE优势**：通过动态平衡偏差与方差，提升策略梯度估计的稳定性，是PPO、A3C等算法的关键组件。
