### 一、优势函数（Advantage Function）

探讨优势函数的背景是，基于策略的优化算法以策略梯度定理为核心，通过调整策略网络的参数，实现效果优化。但原始形式（如REINFORCE）直接使用轨迹回报 R(τ) 作为动作价值的估计，存在高方差问题。为解决这一问题，研究者引入优势函数，并进一步提出改进方法（如GAE），以平衡梯度估计的偏差与方差，从而提升策略优化的效率和稳定性。

优势函数 A<sub>π</sub>(s, a) 衡量在给定状态s下，执行某个动作a，相对于当前策略π的平均收益的优势。公式如下：  

A<sub>π</sub>(s, a) = Q<sub>π</sub>(s, a) - V<sub>π</sub>(s)

其中：

- Q<sub>π</sub>(s, a)：动作价值函数，表示在状态 s 下执行动作 a 的期望累积奖励。
- V<sub>π</sub>(s)：状态价值函数，表示在状态 s 下按当前策略行动的平均期望奖励。

**物理意义**：

- **正值**：动作 a 优于平均水平，应被鼓励。
- **负值**：动作 a 劣于平均水平，应被抑制。

**作用**：

- **减少方差**：通过对比动作与平均水平的差异，降低策略梯度估计的方差。
- **指导策略更新**：优化算法（如PPO、A2C）利用优势函数调整策略，使其更倾向于选择高优势动作。

## 二、传统优势估计方法的局限性

### 1. 蒙特卡洛估计（MC）

A<sub>MC</sub>(s<sub>t</sub>, a<sub>t</sub>) = Σ<sub>k=0</sub><sup>T-t-1</sup> γ<sup>k</sup> r<sub>t+k+1</sub> - V<sub>π</sub>(s<sub>t</sub>)

- **特点**：无偏但高方差（依赖完整轨迹的随机回报）。

- **问题**：在长序列任务中，方差可能爆炸，导致训练不稳定。

### 2. 时序差分估计（TD）

单步TD的优势函数：

A<sub>TD(0)</sub>(s<sub>t</sub>, a<sub>t</sub>) = r<sub>t+1</sub> + γ V<sup>π</sup>(s<sub>t+1</sub>) - V<sup>π</sup>(s<sub>t</sub>)

- **特点**：低方差但有偏（依赖V函数的近似误差）。

- **问题**：若V函数估计不准确，优势函数可能偏离真实值。

## 三、AC及其变种算法的优势函数

#### 1. 基础AC算法（Actor-Critic）

**核心机制**：

- **Actor（策略网络）**：输出动作概率分布，通过策略梯度更新参数。
- **Critic（价值网络）**：评估状态价值 V<sub>s</sub>，结合优势函数指导Actor更新。

**优势函数应用**：

- 通过 A<sub>s,a</sub> = Q<sub>s,a</sub> - V<sub>s</sub> 计算动作优势，优化策略梯度方向。

**特点**：

- 结合策略梯度与价值函数，平衡探索与利用。
- 适用于离散和连续动作空间，但传统AC方差较高。

#### 2. A2C（Advantage Actor-Critic）

**改进点**：

- **同步并行**：多个Worker同步更新全局网络，提高数据利用率。
- **优势函数**：通过 A<sub>s,a</sub> = Q<sub>s,a</sub> - V<sub>s</sub> 减少方差，提升稳定性。

**特点**：

- 适用于离散动作空间（如Atari游戏）。
- 代码实现中，通过多线程并行采样，同步更新参数。

#### 3. A3C（Asynchronous Advantage Actor-Critic）

**改进点**：

- **异步并行**：多个Worker异步更新全局网络，加速训练。
- **探索效率**：不同Worker探索不同路径，避免局部最优。

**特点**：

- 硬件友好，充分利用多核CPU资源。
- 适用于高维度环境（如机器人控制）。

#### 4. DDPG（Deep Deterministic Policy Gradient）

**改进点**：

- **确定性策略**：输出连续动作，结合Q函数估计优势。
- **经验回放与目标网络**：稳定训练过程，处理连续动作空间。

**优势函数应用**：

- Critic通过TD误差更新Q值，间接计算优势函数。

**特点**：

- 专为连续动作空间设计（如机械臂控制）。
- 稳定训练，但样本效率较低。

#### 5. PPO（Proximal Policy Optimization）

**改进点**：

- **策略剪辑**：通过限制策略更新步长（如 clip(π<sub>new</sub>/π<sub>old</sub>, 1-ε, 1+ε) ），避免过大更新。
- **优势函数**：结合GAE估计优势，平衡偏差与方差。

**特点**：

- 稳定性高，适用于复杂任务（如自动驾驶）。
- 超参数敏感（如 ε 值需精细调整）。

#### 6. SAC（Soft Actor-Critic）

**改进点**：

- **熵正则化**：在奖励函数中加入熵项，鼓励探索。
- **双Q网络**：减少Q值高估问题。

**优势函数应用**：

- 优势函数中包含熵项，平衡探索与利用。

**特点**：

- 适用于连续动作空间，探索效率高。
- 调参复杂，但开源实现成熟（如OpenAI Baselines）。

## 四、广义优势估计（GAE）

**广义优势评估（Generalized Advantage Estimation, GAE）** 是强化学习中一种用于高效、低方差地估计状态-动作对优势函数（Advantage Function）的技术，由John Schulman等人在2016年提出。其核心思想通过**混合时序差分（TD）误差与蒙特卡洛（MC）估计**，在偏差（Bias）和方差（Variance）之间取得平衡，从而提升策略梯度方法的性能。

#### 1. 核心原理

**定义**：  

GAE通过多步时间差分（TD）误差的加权平均，平衡优势函数估计的偏差与方差。数学表达式为：  

A<sub>t</sub> = Σ<sub>k=0</sub><sup>∞</sup> (γλ)<sup>k</sup> δ<sub>t+k</sub>  

其中，δ<sub>t</sub> = r<sub>t</sub> + γV(s<sub>t+1</sub>) - V(s<sub>t</sub>)  是单步TD误差，γ 控制未来奖励权重，λ 控制偏差与方差的平衡。这里采用的是0到正无穷的累加形式，但在实际应用中，有可能提前遇到终止步，终止状态的状态价值函数为0，可以从终止状态往前递推，进而求解各步骤的优势值。

**关键推导**：  

- **多步优势估计**：n步优势估计为 A<sub>t</sub><sup>(n)</sup> = Σ<sub>k=0</sub><sup>n-1</sup> γ<sup>k</sup> δ<sub>t+k</sub>。  

- **广义加权平均**：GAE将不同步数的优势估计通过 λ 加权组合，递归形式为：  
  
  A<sub>t</sub> = δ<sub>t</sub> + γλA<sub>t+1</sub>

#### 2. 参数作用

- **λ**：  
  
  - λ → 1：方差大，偏差小（接近蒙特卡洛估计）。  
  - λ → 0：方差小，偏差大（接近单步TD估计）。  
  - 实际中，λ 常取 0.9～0.99。

- **γ**：  
  
  - 控制未来奖励的折扣因子，直接影响价值函数的估计。

#### 3. 实际应用

**策略梯度更新**：  

∇J(θ) = E[ Σ<sub>t=0</sub><sup>T</sup> A<sub>t</sub><sup>GAE</sup> ∇<sub>θ</sub> <i>ln</i> π<sub>θ</sub>(a<sub>t</sub> | s<sub>t</sub>) ]  

GAE替代原始优势函数，提供更稳定的梯度估计。

**优势归一化**：  

为减少方差，常对GAE估计进行标准化：  

Â<sub>t</sub> = (A<sub>t</sub><sup>GAE</sup> - μ<sub>A</sub>) / σ<sub>A</sub>  

其中，μ<sub>A</sub> 和 σ<sub>A</sub> 为批量数据的均值和标准差。

#### 4. 伪代码示例

```Python
def compute_gae(rewards, values, gamma=0.99, lambda_=0.95):
    batch_size = len(rewards)
    advantages = [0] * batch_size
    last_advantage = 0
    for t in reversed(range(batch_size)):
        delta = rewards[t] + gamma * values[t+1] - values[t]
        advantages[t] = delta + gamma * lambda_ * last_advantage
        last_advantage = advantages[t]
    return advantages
```

## 五、总结

- **优势函数共性**：所有变种均通过优势函数评估动作质量，核心差异在于并行机制、策略类型（确定性/随机性）及更新约束。  
- **选择建议**：  
  - **离散动作**：优先A2C/A3C。  
  - **连续动作**：DDPG或SAC。  
  - **高稳定性需求**：PPO。  
  - **资源受限环境**：A3C（多核CPU）。  
- **GAE优势**：通过动态平衡偏差与方差，提升策略梯度估计的稳定性，是PPO、A3C等算法的关键组件。
