# AC架构

Actor-Cristic架构简称AC架构，可称为演员-裁判架构或演员-评委架构，被广泛用于多种优化算法中，例如PPO、DPG、DDPG、TD3等。AC架构结合了基于策略和基于价值的算法的优势，通过同时训练价值函数和策略函数，提升了训练的稳定性和学习效率。

## 一、核心原理

### 1.1 架构组成

**Actor-Critic架构**是一种结合策略梯度（Policy Gradient）和价值函数（Value Function）的强化学习框架，由两部分组成：

- **Actor（策略网络）**：
  
  - **功能**：负责生成动作的概率分布 $\pi_\theta(a \mid s)$，其中 $\theta$ 为网络参数。Actor通过策略梯度方法更新参数，以最大化长期回报。
  
  - **更新方式**：利用Critic提供的TD误差作为优势函数，调整自身网络权重，改变策略梯度，使得高优势动作的概率增加。具体公式为：
    
    <span class="math-display">θ ← θ + α<sub>θ</sub> δ<sub>t</sub> ∇<sub>θ</sub> <i>ln</i> π(a<sub>t</sub> | s<sub>t</sub>; θ)</span>

    其中，δ<sub>t</sub> 为TD误差，α<sub>θ</sub> 为Actor的学习率。

- **Critic（价值网络）**：
  
  - **功能**：评估状态或状态-动作对的价值 $V_w(s)$ 或 $Q_w(s, a)$，其中 $w$ 为网络参数。Critic通过时序差分（TD）误差更新参数，以逼近真实值函数。
  
  - **更新方式**：最小化TD误差的均方误差，优化值函数预测。具体公式为：
 
    <span class="math-inline"> w ← w + α<sub>w</sub> δ<sub>t</sub> ∇<sub>w</sub> V(s<sub>t</sub>, w) </span>

    其中，<span class="math-inline">δ<sub>t</sub></span> 为Critic的学习率。

### 1.2 交互原理

1. **Actor选择动作**：Actor网络与环境进行交互，根据当前状态 $s$，按照策略函数 $\pi(a \mid s; \theta)$ 选择一个动作 $a_t$，并得到及时奖励 $r(t)$ 和下一个状态 $s_{t+1}$。

2. **Critic评估价值**：Critic计算当前状态的价值 $V(s_t)$ 和下一个状态的价值 $V(s_{t+1})$，并根据及时奖励 $r(t)$，得到TD误差：
   
   <span class="math-inline"> δ<sub>t</sub> = r<sub>t+1</sub> + γV(s<sub>t+1</sub>) - V(s<sub>t</sub>) </span>
   
   其中，<span class="math-inline">$\gamma$</span>  为折扣因子。这个TD误差是一个关于动作 $a$ 好坏的反馈，误差越小，代表该动作的收益就越大，Actor网络应该增大该动作的概率。

3. **Actor更新**：Actor网络接收到TD误差，作为优势函数的近似，调整自身网络权重 $\theta$，改变策略梯度，使得高优势动作的概率增加。

4. **Critic更新**：最后Critic网络更新，最小化TD误差，优化值函数预测。

## 二、算法流程

### 2.1 初始化

- 初始化策略网络（Actor）和价值网络（Critic）及其参数。
- 设置优化器和超参数（学习率、折扣因子 $\gamma$、隐藏层大小等）。

### 2.2 交互采样

- 在每个训练回合，从环境中采样一条轨迹：
  - 根据当前策略 $\pi_\theta(a \mid s)$ 选择动作 $a$。
  - 执行动作，获得奖励 $r$ 和下一个状态 $s'$。
  - 判断是否到达终止状态。

### 2.3 计算时序差分（TD）目标

- 计算TD目标：
  
  <span class="math-inline">TD Target = r + γV(s')</span>

- 计算TD误差：
  
  <span class="math-inline">δ = r + γV(s') - V(s)</span>

### 2.4 更新Critic（价值网络）

- 通过最小化TD误差的均方误差（mean-square error, MSE），更新Critic的参数：
  
  <span class="math-inline">L(φ) = (δ)<sup>2</sup></span>

### 2.5 更新Actor（策略网络）

使用策略梯度方法更新Actor的参数。注意，不同的算法对价值部分有不同的定义，梯度策略中默认使用路径回报R(τ)，DQN使用价值模型来近似动作价值函数 Q<sub>w</sub>(s<sub>t</sub>, a<sub>t</sub>)，而优势AC算法使用相对优势函数。

∇<sub>θ</sub>J(θ) = log π<sub>θ</sub>(a|s) · δ

### 2.6 重复训练

- 不断采样数据并更新Actor和Critic，直到策略收敛或达到预设的训练回合。

## 三、优势与挑战

### 3.1 优势

- **降低方差**：Critic提供的值函数估计替代蒙特卡洛回报，减少策略梯度的方差。
- **实时反馈**：Critic在线评估动作价值，加速策略收敛。
- **灵活性**：支持离散/连续动作空间，适用于复杂环境（如机器人控制、游戏AI）。
- **稳定性优于纯策略梯度**：通过Critic的引入，Actor-Critic在复杂环境中比策略梯度算法表现更稳定。

### 3.2 挑战

- **耦合风险**：Actor和Critic的更新需同步，否则可能导致训练不稳定。
- **偏差-方差权衡**：Critic的估计误差会引入偏差，需设计平衡机制（如优势函数）。
- **样本效率低**：Actor-Critic算法仍然需要大量交互数据，尤其是在高维状态空间中，样本效率较低。
- **训练不稳定**：Actor和Critic的更新是相互依赖的，Critic估计的错误可能会影响Actor的更新，从而导致训练振荡。

## 四、变种形式

### 4.1 A2C

- Advantage Actor-Critic，优势AC算法

- 同步多线程采样，使用优势函数 $A_w(s, a) = Q_w(s, a) - V_w(s)$ 替代Q值，减少方差，提升学习效果。

### 4.2 A3C

- Asynchronous Advantage Actor-Critic，异步优势AC算法

- 异步多线程采样，提升数据效率，增强稳定性。

### 4.3 DDPG

- Deep Deterministic Policy Gradient，深度确定性策略梯度

- 面向连续动作空间，Actor输出确定性策略，Critic评估Q值，使用经验回放和目标网络稳定训练过程。

### 4.4 PPO

- Proximal Policy Optimization，近端策略优化

- 引入“剪切”损失函数，限制策略更新的步长，避免过大的更新带来的不稳定性，可看作是Actor-Critic方法的一种改进。

## 五、应用场景

- 机器人控制：帮助机器人在连续状态和动作空间中进行路径规划和任务执行，如机械臂操控、自动驾驶等。

- 游戏AI：训练游戏中的智能体，使其能够在动态环境中做出最佳决策，如Atari游戏、星际争霸等。

- 金融交易：在不确定的市场条件下进行动态投资决策，以实现最大化收益。

- 其他领域：如推荐系统、资源管理等，通过Actor-Critic架构优化决策过程，提升系统性能。

## 六、代码示例（PyTorch）

以下是一个使用PyTorch实现的Actor-Critic算法代码示例，应用于经典的CartPole-v0问题：

```python
import gym
import torch
import torch.nn.functional as F
import numpy as np

# 定义策略网络（Actor）
class PolicyNet(torch.nn.Module):
    def __init__(self, state_dim, hidden_dim, action_dim):
        super(PolicyNet, self).__init__()
        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)
        self.fc2 = torch.nn.Linear(hidden_dim, action_dim)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        return F.softmax(self.fc2(x), dim=1)

# 定义价值网络（Critic）
class ValueNet(torch.nn.Module):
    def __init__(self, state_dim, hidden_dim):
        super(ValueNet, self).__init__()
        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)
        self.fc2 = torch.nn.Linear(hidden_dim, 1)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        return self.fc2(x)

# 训练参数
state_dim = 4
hidden_dim = 128
action_dim = 2
learning_rate_actor = 1e-3
learning_rate_critic = 1e-3
gamma = 0.99

# 初始化网络和优化器
actor = PolicyNet(state_dim, hidden_dim, action_dim)
critic = ValueNet(state_dim, hidden_dim)
optimizer_actor = torch.optim.Adam(actor.parameters(), lr=learning_rate_actor)
optimizer_critic = torch.optim.Adam(critic.parameters(), lr=learning_rate_critic)

# 训练过程
env = gym.make('CartPole-v0')
for episode in range(1000):
    state = env.reset()
    while True:
        # Actor选择动作
        state_tensor = torch.FloatTensor(state).unsqueeze(0)
        action_probs = actor(state_tensor)
        action = np.random.choice(action_dim, p=action_probs.squeeze(0).detach().numpy())

        # 执行动作，获取新状态和奖励
        next_state, reward, done, _ = env.step(action)

        # 计算TD误差
        next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0)
        current_value = critic(state_tensor)
        next_value = critic(next_state_tensor) if not done else torch.zeros(1, 1)
        td_error = reward + gamma * next_value - current_value

        # 更新Critic
        critic_loss = td_error.pow(2).mean()
        optimizer_critic.zero_grad()
        critic_loss.backward()
        optimizer_critic.step()

        # 更新Actor
        actor_loss = -torch.log(action_probs.squeeze(0)[action]) * td_error.squeeze(0)
        optimizer_actor.zero_grad()
        actor_loss.backward()
        optimizer_actor.step()

        # 更新状态
        state = next_state

        if done:
            break
```

这段代码展示了如何使用PyTorch实现Actor-Critic算法，通过同时训练策略网络（Actor）和价值网络（Critic）来优化策略。在CartPole-v0环境中，算法通过控制推力来保持杆子直立，通过不断的交互和更新，最终学习到稳定的平衡策略。
