## 一、PPO的概念

PPO（Proximal Policy Optimization），即近端策略优化，是一种**策略梯度强化学习算法**，旨在通过限制策略更新的幅度，平衡探索与利用，解决传统策略梯度方法（如REINFORCE）训练不稳定、样本效率低的问题。其核心思想是：**在优化策略时，避免新旧策略差异过大导致性能崩溃**，通过“近端”约束实现稳定学习。

### 关键公式

1. **裁剪目标函数（Clipped Surrogate Objective）**  
   PPO的核心是通过裁剪策略比值（新旧策略的概率比）来限制更新幅度：  
   ![](./image/2025-07-07-21-23-00.png)
   
   其中：  
   
   - <i>π<sub>θ</sub></i>和<i>π</i><sub><i>θ_old</i></sub>分别为新旧策略，  
   - <i>Â<sub>t</sub></i>为优势函数估计（如GAE），  
   - ε为裁剪系数（通常取0.1~0.3），控制更新幅度。

2. **优势函数估计（GAE）**  
   PPO使用广义优势估计（GAE）平衡偏差与方差：  
   
   <i>Â<sub>t</sub></i> = Σ<sub>l=0</sub><sup>∞</sup> (γλ)<sup>l</sup>δ<sub>t+l</sub><sup>V</sup>  
   
   其中：  
   
   - γ为折扣因子，λ为GAE平滑系数，  
   - δ<sub>t</sub><sup>V</sup> = r<sub>t</sub> + γV(s<sub>t+1</sub>) - V(s<sub>t</sub>) 为TD误差，  
   - V(s)为状态值函数。

## 二、PPO相关算法的演进

### 1. 策略梯度方法的起源

- **REINFORCE（Williams, 1992）**  
  
  最早的策略梯度方法，通过蒙特卡洛采样估计梯度，但方差高、训练不稳定。
  
  梯度公式：
  
  ∇<sub>θ</sub>J(θ) = 𝔼[Σ<sub>t=0</sub><sup>T</sup> ∇<sub>θ</sub>log<i>π<sub>θ</sub>(a<sub>t</sub>|s<sub>t</sub>)</i> * G<sub>t</sub>]
  
  其中 G<sub>t</sub>为累积回报。

- **Actor-Critic（1990s）**  
  引入价值函数（Critic）估计优势函数，降低方差，但需同时训练策略（Actor）和价值函数（Critic），增加了复杂性。

### 2. 置信域策略优化（TRPO, 2015）

- **核心思想**：通过约束新旧策略的KL散度，确保更新在“信任域”内： 
  
  maximize 𝔼[<i>Â<sub>t</sub></i> * (<i>π<sub>θ</sub>(a|s)</i> / <i>π<sub>θ__old</sub>(a|s)</i>)]  
  s.t. 𝔼[KL(<i>π<sub>θ__old</sub></i>, <i>π<sub>θ</sub></i>)] ≤ δ  

- **问题**：TRPO需通过共轭梯度法求解约束优化问题，计算复杂度高，难以扩展到大规模神经网络。

### 3. PPO的提出（2017）

- **改进TRPO**：PPO用简单的裁剪目标函数替代KL约束，避免复杂优化，同时保留了更新幅度的限制。  
- **优势**：  
  - 计算效率高（仅需一阶优化），  
  - 易于实现（基于标准策略梯度框架），  
  - 在连续和离散控制任务中表现优异。

### 4. PPO的变体与扩展

- **PPO-Penalty（2017）**  
  用KL散度惩罚项替代裁剪：  
  <i>L<sup>KL</sup>(θ)</i> = 𝔼[<i>Â<sub>t</sub></i> * (<i>π<sub>θ</sub>(a|s)</i> / <i>π<sub>θ__old</sub>(a|s)</i>)] - β * KL(<i>π<sub>θ__old</sub></i>, <i>π<sub>θ</sub></i>)  
  其中β为动态调整的惩罚系数。

- **分布式PPO（DPPO, 2017）**  
  通过多进程并行采集数据，加速训练，适用于大规模任务（如机器人控制、游戏AI）。

- **PPO+GAE（2018）**  
  结合广义优势估计（GAE）进一步降低方差，成为PPO的默认配置。

### 5. 现代改进方向

- **自适应裁剪系数**：根据训练动态调整ε，提升灵活性。  
- **状态表示学习**：结合对比学习或自监督学习，提升策略对复杂环境的感知能力。  
- **离线强化学习**：将PPO扩展到离线场景（如PPO-Offline），利用静态数据集训练策略。

## 三、TRPO

### 1. 算法核心思想

TRPO（Trust Region Policy Optimization，置信域策略优化）是一种基于策略梯度的强化学习算法，其核心目标是通过**置信域约束**解决传统策略梯度方法中步长选择困难的问题。通过限制新旧策略的差异幅度，确保每次更新在安全范围内，从而实现稳定的策略优化。

### 2. 置信域与策略约束的数学原理

#### 2.1 置信域的定义

置信域（Trust Region）是指在策略参数空间中定义的一个区域，要求新策略与旧策略的差异不超过预设阈值。TRPO通过**KL散度**（Kullback-Leibler Divergence）衡量策略差异，并施加约束：

**KL散度约束公式**：

Es∼πθ[DKL(πθold(·|s) || πθ(·|s))] ≤ δ

- **解释**：  
  \(D_{KL}(π_{old} || π_{new})\) 表示旧策略 \(π_{old}\) 与新策略 \(π_{new}\) 在状态 \(s\) 下的动作分布差异。  
  \(δ\) 为预设阈值，控制策略更新的最大允许幅度。

### 2. 策略优化的目标函数

TRPO的目标是最大化期望优势函数，同时满足KL散度约束：

**目标函数**：

max<sub>θ</sub> E<sub>s,a∼π<sub>θ<sub>old</sub></sub></sub> [ (π<sub>θ</sub>(a|s) / π<sub>θ<sub>old</sub></sub>(a|s)) * A<sup>π<sub>θ<sub>old</sub></sub></sup>(s,a) ]

```html

```

- **解释**：  
  \(A^{π_{old}}(s,a)\) 为优势函数，衡量动作 \(a\) 在状态 \(s\) 下的相对价值。  
  通过重要性采样比率 \(\frac{π_{new}}{π_{old}}\) 估计策略梯度，避免直接采样新策略。

### 3. 约束优化问题的近似求解

直接求解上述约束优化问题困难，TRPO采用泰勒展开近似：

**泰勒展开近似**：

```html
max<sub>θ</sub> g<sup>T</sup>(θ - θ<sub>old</sub>)  s.t.  (1/2)(θ - θ<sub>old</sub>)<sup>T</sup> H (θ - θ<sub>old</sub>) ≤ δ
```

- **解释**：  
  \(g\) 为策略梯度的一阶近似，\(H\) 为KL散度的二阶Hessian矩阵。  
  通过共轭梯度法高效求解 \(H^{-1}g\)，避免直接计算Hessian矩阵的逆。

## 三、算法流程与关键步骤

### 1. 算法流程

1. **数据采样**：使用当前策略 \(π_{old}\) 与环境交互，收集轨迹数据。
2. **计算优势函数**：基于轨迹数据，计算每个状态-动作对的优势值。
3. **策略梯度估计**：计算目标函数的梯度 \(g\)。
4. **共轭梯度求解**：近似计算自然梯度方向 \(x ≈ H^{-1}g\)。
5. **回溯线搜索**：确定满足KL约束的最大步长，更新策略参数。
6. **价值函数更新**：优化价值网络以更准确地估计状态价值。

### 2. 关键公式与步骤详解

- **共轭梯度法**：  
  用于高效求解线性方程 \(Hx = g\)，避免直接计算Hessian矩阵的逆，显著降低计算复杂度。

- **回溯线搜索**：  
  通过逐步缩小步长，确保更新后的策略满足KL散度约束：
  
  ```html
  θ<sub>new</sub> = θ<sub>old</sub> + α<sup>j</sup> * x
  ```
  
  其中 \(α\) 为回溯系数，\(j\) 为回溯步数。

## 四、TRPO的优势与局限性

### 1. 优势

- **单调性能提升**：通过KL约束保证策略性能不会突然下降，形成单调递增的策略序列。
- **大步长更新**：相比传统策略梯度，允许更大的学习步长，加速收敛。
- **稳定性与效率平衡**：在探索激进性与计算复杂度之间取得平衡，适用于高维连续动作空间。

### 2. 局限性

- **计算复杂度高**：需要计算二阶信息（Hessian矩阵），实现复杂。
- **探索不足**：作为on-policy方法，可能陷入局部最优。

## 五、总结

TRPO通过**置信域约束**和**KL散度控制**，解决了传统策略梯度方法中步长选择困难的问题，实现了稳定且高效的策略优化。其核心思想在后续算法（如PPO）中得到进一步简化与应用，成为强化学习领域的重要基石。

**公式汇总**：

1. **KL散度约束**：  
   
   ```html
   E<sub>s∼π<sub>θ</sub></sub>[D<sub>KL</sub>(π<sub>θ<sub>old</sub></sub>(·|s) || π<sub>θ</sub>(·|s))] ≤ δ
   ```
2. **目标函数**：  
   
   ```html
   max<sub>θ</sub> E<sub>s,a∼π<sub>θ<sub>old</sub></sub></sub> [ (π<sub>θ</sub>(a|s) / π<sub>θ<sub>old</sub></sub>(a|s)) * A<sup>π<sub>θ<sub>old</sub></sub></sup>(s,a) ]
   ```
3. **泰勒展开近似**：  
   
   ```html
   max<sub>θ</sub> g<sup>T</sup>(θ - θ<sub>old</sub>)  s.t.  (1/2)(θ - θ<sub>old</sub>)<sup>T</sup> H (θ - θ<sub>old</sub>) ≤ δ
   ```
