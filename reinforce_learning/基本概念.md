强化学习的核心概念可归纳为以下四大类，涵盖其理论框架与关键机制：

### 一、基础元素

1. **智能体（Agent）**
   - 主动学习者，通过执行动作与环境交互，基于反馈优化决策策略。
2. **环境（Environment）**
   - 动态系统，定义状态转移规则与奖励机制，响应智能体动作并返回新状态与奖励。
3. **状态（State, s）与观测（Observation）**
   - **完全可观测**：智能体获取环境全部信息（如棋盘布局）。
   - **部分可观测**：仅能通过传感器等获取局部信息（如机器人视野）。
4. **动作（Action, a）**
   - 智能体在特定状态下可采取的操作集合，直接影响环境状态变化。
5. **奖励（Reward, r）**
   - 环境对动作的即时反馈，引导智能体趋向目标（如游戏得分、任务完成度）。

### 二、学习目标与策略

1. **策略（Policy, π）**
   决策规则，定义状态到动作的映射：
   
   - **确定性策略**：a=π(s)（如自动驾驶中的固定路线）。
   - **随机性策略**：π(a∣s)（如探索未知区域时的概率选择）。

2. **回报（Return, G<sub>t</sub>​）**
   
   - 未来奖励的加权总和，公式为
   
   G<sub>t</sub>=r<sub>t+1</sub>+γr<sub>t+2</sub>+γ<sup>2</sup>r<sub>t+3</sub>+⋯    
   
   其中 γ（折扣因子）平衡即时收益与长期收益，值越小越看重即时收益。

3. **价值函数（Value Function）**
   
   价值是回报的期望
   
   - **状态价值 V<sub>π</sub>(s)**：从状态 s 开始遵循策略 π 的期望累积奖励。
   - **动作价值 Q<sub>π</sub>(s,a)**：执行动作 a 后遵循策略 π 的期望收益，指导动作选择。

### 三、数学框架与扩展

1. 马尔可夫性
   
   - 系统未来状态的条件概率仅依赖于当前状态，与历史状态无关。
     
     P(s<sub>t+1</sub>​∣s<sub>t</sub>,s<sub>t−1</sub>,…,s<sub>0</sub>)=P(s<sub>t+1</sub>∣s<sub>t</sub>)
   
   - 齐次性：状态之间的转移概率不随时间变化的性质。齐次性是马尔可夫链的**额外假设**，并非马尔可夫性的必然结果，有助于简化部分问题。

2. **马尔可夫决策过程（MDP）**
   
   - 强化学习的标准模型，由五元组 (S,A,P,R,γ) 定义：
     - S：状态空间
     - A：动作空间
     - P(s′∣s,a)：状态转移概率
     - R(s,a,s′)：奖励函数。
     - 假设满足“马尔可夫性”（未来仅依赖当前状态与动作）。

3. **部分可观测马尔可夫决策过程（POMDP）**
   
   - 扩展MDP框架，适用于状态无法完全观测的场景（如机器人通过模糊传感器感知环境）。

### 四、训练机制与技巧

1. **探索与利用（Exploration vs. Exploitation）**
   
   - **探索**：尝试新动作以发现更高奖励的策略（如随机选择动作）。
   - **利用**：选择已知最优动作以最大化当前收益（如重复成功经验）。
   - **平衡方法**：ϵ-贪心策略（以概率 ϵ 随机探索）、上置信界（UCB）算法。

2. **经验回放（Experience Replay）**
   
   - 存储历史交互数据（状态-动作-奖励-新状态），随机采样训练以打破数据相关性（如DQN算法）。

3. 同策略和异策略
   
   策略是智能体的核心，决定了智能体在面对不同情况时采取什么动作。这里需要区分行为策略（behavior policy）和目标策略（target policy）：
   
   - 行为策略：智能体在探索环境、收集训练数据时，和环境交互过程中所采取的交互行为策略。
   - 目标策略：智能体最终采纳的策略，通常以最大化回报为目标。
   - 同策略/异策略：根据采集经验/数据时和训练时是否采用同一个策略，分为同策略（on-policy）和异策略（off-policy）
   - 重要性采样：异策略的行为策略和目标策略不同，可能导致在数据在分布上有偏差，因此需要通过重要性采样权重矫正期望估计

4. 在线和离线
   
   根据学习过程中，智能体是否直接和实际环境交互，分为在线强化学习（Online RL）和离线强化学习（Offline RL）。离线强化学习都是异策略。

5. **目标网络（Target Network）**
   
   - 在Q-learning中，使用独立网络计算目标Q值，减少训练波动（如DQN的固定目标网络）。

### 示例场景

- **游戏AI**：AlphaGo通过棋盘状态（状态）选择落子（动作），根据胜负结果（奖励）优化策略。
- **智能驾驶**：车辆（智能体）通过传感器感知部分环境（部分可观测状态）。车（智能体）可以根据感知信息判定，依据一定规则（策略）判断是否需要转向、掉头等（动作），车和道路（环境）交互后 确定了车的新位置、方向、速度等（奖励）。去目的地的路线有很多，车辆可以选择已知的路径（利用）， 也可以走新的 路径（探索），每一次操作选择都会产生一定影响（奖励），这个影响对车辆后续的状态会带来持续的影响（回报）。

这些概念共同构建了强化学习的理论体系，支撑从简单控制到复杂决策问题的解决方案。实际应用中需结合具体场景选择算法（如Q-learning、Policy Gradient）与超参数（如学习率、折扣因子）。
